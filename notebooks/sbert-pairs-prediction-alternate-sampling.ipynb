{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install --upgrade -q torch\n",
    "!pip install --quiet -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "# Making string because someone had number in question and pandas read it as float ;-)\n",
    "train.question1 = train.question1.apply(str)\n",
    "train.question2 = train.question2.apply(str)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 404290\n",
      "Unique first questions: 290654\n",
      "Unique second questions: 299364\n",
      "Unique questions: 537933\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows: %d\\nUnique first questions: %d\\nUnique second questions: %d\\nUnique questions: %d\" % (len(train), train.qid1.nunique(), train.qid2.nunique(), len(np.unique([train.qid1, train.qid2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.1\n",
    "train, val = train[:int(len(train) * (1 - val_ratio))], train[int(len(train) * (1 - val_ratio)):]\n",
    "val = val.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForNextSentencePrediction\n",
    "\n",
    "\n",
    "def tokenizer_to_device(tokenizer, device):\n",
    "    def deviced(text, return_tensors='pt', padding=True):\n",
    "        tokens = tokenizer(text, padding=padding, return_tensors=return_tensors)\n",
    "        tokens = {\n",
    "            'input_ids': tokens['input_ids'].to(device),\n",
    "            'token_type_ids': tokens['token_type_ids'].to(device),\n",
    "            'attention_mask': tokens['attention_mask'].to(device)\n",
    "        }\n",
    "        return tokens\n",
    "    return deviced\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "tokenizer = tokenizer_to_device(bert_tokenizer, device)\n",
    "\n",
    "config = BertConfig(\n",
    "    output_attentions=True, \n",
    "    output_hidden_states=True, \n",
    "    return_dict=True)\n",
    "model = BertForNextSentencePrediction.from_pretrained(model_name, config=config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def predict(sentences, tokenizer, model, device, batch_size=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_logits = []\n",
    "        sentences = [*sentences]\n",
    "\n",
    "        batch_n = ceil(len(sentences) / batch_size)\n",
    "        for batch_i in range(batch_n):\n",
    "            tokens = tokenizer(sentences[batch_i*batch_size:(batch_i+1)*batch_size])\n",
    "            prediction = model(**tokens)\n",
    "            logits = torch.nn.functional.softmax(prediction.logits, dim=1)[:, 0]\\\n",
    "                .to(\"cpu\").detach().numpy()\n",
    "            all_logits.append(logits)\n",
    "            del tokens, prediction\n",
    "            print(\"\\rPrediction batch %5d / %5d\\t\\t\\t\\t\\t\\t\\t\" % (batch_i + 1, batch_n), end='')\n",
    "\n",
    "        print(\"\\r\", end='')\n",
    "        return np.concatenate(all_logits)\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    positive_samples = defaultdict(list)\n",
    "    negative_samples = defaultdict(list)\n",
    "    text_by_id = {}\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        if row.is_duplicate:\n",
    "            positive_samples[row.qid1].append(row.qid2)\n",
    "            positive_samples[row.qid2].append(row.qid1)\n",
    "        else:\n",
    "            negative_samples[row.qid1].append(row.qid2)\n",
    "            negative_samples[row.qid2].append(row.qid1)\n",
    "        text_by_id[row.qid1] = row.question1\n",
    "        text_by_id[row.qid2] = row.question2\n",
    "        print(\"\\rPreprocessing %10d / %10d\" % (i + 1, len(data)), end='')\n",
    "        \n",
    "    question_ids = [*set(positive_samples.keys()).intersection(set(negative_samples.keys()))]\n",
    "    print(\"\\rPreprocessing done, unique valid questions: %d\" % len(question_ids))\n",
    "    \n",
    "    return {'pos': positive_samples, 'neg': negative_samples, 'text_by_id': text_by_id, 'question_ids': question_ids}\n",
    "    \n",
    "\n",
    "def train_triplet(model, tokenizer, preprocessed_data, optimizer, triplet_loss, n_epochs=1, epoch_coverage=1, batch_size=16, sbert=False):\n",
    "    positive_samples, negative_samples, text_by_id, question_ids = \\\n",
    "        preprocessed_data['pos'], preprocessed_data['neg'], preprocessed_data['text_by_id'], preprocessed_data['question_ids']\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch_i in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        size = ceil(len(question_ids) * epoch_coverage / batch_size) * batch_size\n",
    "        n_batches = ceil(size / batch_size)\n",
    "        \n",
    "        for batch_i in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            anchor_ids = np.random.choice(question_ids, batch_size, replace=False)\n",
    "            pos_ids = [np.random.choice(positive_samples[id], 1)[0] for id in anchor_ids]\n",
    "            neg_ids = [np.random.choice(negative_samples[id], 1)[0] for id in anchor_ids]\n",
    "            \n",
    "            ids = [*anchor_ids, *pos_ids, *neg_ids]\n",
    "            sentences = [text_by_id[i] for i in ids]\n",
    "            tokens = tokenizer(sentences)\n",
    "            if sbert:\n",
    "                embeddings = model(tokens)['sentence_embedding']\n",
    "            else:\n",
    "                embeddings = model(**tokens).logits\n",
    "            anchor, pos, neg = embeddings[:batch_size], embeddings[batch_size:batch_size*2], embeddings[batch_size*2:]\n",
    "            \n",
    "            loss = triplet_loss(anchor, pos, neg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_loss = loss.detach().cpu().item()\n",
    "            epoch_loss += batch_loss\n",
    "            print(\"\\rBatch %3d / %3d, Batch loss: %5.3f, Epoch loss: %5.3f\" %\n",
    "                     (batch_i + 1, n_batches, batch_loss, epoch_loss / (batch_i + 1)), end='')\n",
    "        \n",
    "        epoch_loss /= n_batches\n",
    "        print(\"\\rEpoch %3d / %3d, Loss: %5.3f%s\" % (epoch_i + 1, n_epochs, epoch_loss, \"\\t\"*12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done, unique valid questions: 21596\n"
     ]
    }
   ],
   "source": [
    "prep_train = preprocess_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = torch.nn.TripletMarginLoss(margin=1.0)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 /  10, Loss: 0.994\t\t\t\t\t\t, Epoch loss: 0.994\n",
      "Epoch   2 /  10, Loss: 0.931\t\t\t\t\t\t, Epoch loss: 0.931\n",
      "Epoch   3 /  10, Loss: 0.849\t\t\t\t\t\t, Epoch loss: 0.849\n",
      "Epoch   4 /  10, Loss: 0.785\t\t\t\t\t\t, Epoch loss: 0.785\n",
      "Epoch   5 /  10, Loss: 0.791\t\t\t\t\t\t, Epoch loss: 0.791\n",
      "Epoch   6 /  10, Loss: 0.724\t\t\t\t\t\t, Epoch loss: 0.724\n",
      "Epoch   7 /  10, Loss: 0.686\t\t\t\t\t\t, Epoch loss: 0.686\n",
      "Epoch   8 /  10, Loss: 0.669\t\t\t\t\t\t, Epoch loss: 0.669\n",
      "Epoch   9 /  10, Loss: 0.645\t\t\t\t\t\t, Epoch loss: 0.645\n",
      "Epoch  10 /  10, Loss: 0.654\t\t\t\t\t\t, Epoch loss: 0.654\n"
     ]
    }
   ],
   "source": [
    "train_triplet(model, tokenizer, prep_train, optim, triplet_loss,\n",
    "             n_epochs=10, epoch_coverage=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_encodings(df, sbert=False, batch_size=32):\n",
    "    questions = [*df[['qid1', 'question1']].values, *df[['qid2', 'question2']].values]\n",
    "    questions = pd.DataFrame(questions, columns=['id', 'question'])\n",
    "    questions.question = questions.question.apply(str)\n",
    "    questions.drop_duplicates(inplace=True)\n",
    "    questions = questions.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    if sbert:\n",
    "        encod = model.encode(questions.question)\n",
    "    else:\n",
    "        encod = predict(questions.question, tokenizer, model, device, batch_size=batch_size)\n",
    "\n",
    "    encodings = {}    \n",
    "    for i, item in questions.iterrows():\n",
    "        encodings[item.id] = encod[i]\n",
    "    \n",
    "    return np.stack(df['qid1'].apply(lambda x: encodings[x]).values), np.stack(df['qid2'].apply(lambda x: encodings[x]).values)\n",
    "    \n",
    "def calc_threshold(df, encod1, encod2):\n",
    "    distances = np.linalg.norm(encod1 - encod2, axis=1)\n",
    "    \n",
    "    positive_distance = distances[df.is_duplicate].mean()\n",
    "    negative_distance = distances[~df.is_duplicate].mean()\n",
    "    threshold = (positive_distance + negative_distance) / 2\n",
    "\n",
    "    print(\"Positive d: %.3f, Avg d: %.3f, Negative d: %.3f\" % (positive_distance, threshold, negative_distance))\n",
    "    \n",
    "    df['dist'] = distances\n",
    "    return positive_distance, threshold, negative_distance\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def calc_metrics(df, positive_dist, threshold, negative_dist):\n",
    "    pred = sigmoid(-(df['dist'] - threshold) / (negative_distance - positive_distance))\n",
    "    accuracy = np.mean((pred > 0.5) == df.is_duplicate)\n",
    "    log_loss = loss(df.is_duplicate, pred)\n",
    "\n",
    "    print(\"Accuracy: %.5f\" % accuracy)\n",
    "    print(\"Log Loss: %.5f\" % log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer is taking too much space, **needs to be deleted :-(**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del optim\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod1, encod2 = calc_encodings(train, batch_size=32)\n",
    "positive_dist, threshold, negative_dist = calc_threshold(train, encod1, encod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64153\n",
      "Log Loss: 0.66817\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(train, positive_dist, threshold, negative_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive d: 0.474, Avg d: 0.556, Negative d: 0.639\n"
     ]
    }
   ],
   "source": [
    "encod1, encod2 = calc_encodings(val)\n",
    "calc_threshold(val, encod1, encod2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64840\n",
      "Log Loss: 0.66671\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(val, positive_dist, threshold, negative_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
